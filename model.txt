Model 1 (Simplest)
==================

 * No replication
 * No repartitioning

 Let H := { h_1, h_2, ..., h_m } be m hosts in the cluster.
 Let S := { s_1, s_2, ..., s_n } be n sites at hosts.
 
 Define SH(h_i) = { sites at host h_i }
 
 Let SP := { sp_1, sp_2, ..., sp_p } be p stored procedures.
 Let W  := { T_1, T_2, ..., T_w } be w transactions in workload during the epoch.
 
 Notation: For transaction T_i, T_i.sp is its stored procedure in SP.
 Notation: For transaction T_i, T_i.lat is the avg. execution time of stored procedure sp as observed in previous epoch(s).
 
 Simplification 1: All queries using a given stored procedure touch the same set of partitions.
 Q: How to change the model if we observed that stored procedure touched set of partitions s_1 with probability p_1, s_2 with probability p_2, etc. ?
 
 Define SPS(sp) := { sites touched by stored procedure sp }
 
 Assertion 1: If intersection of SPS(TSP(T_i)) and SPS(TSP(T_j)) is non-empty (i.e. they touch a common site or sites), then T_i and T_j execute serially in some order.  

 Ex. Consider a three sites s_1, s_2 and s_3 and a workload with three transactions T_1, T_2 and T_3.
     T_1 uses all sites, T_2 uses site s_1 and T_3 uses site s_3.
     Suppose T_2 and T_3 have latency of 500us and T_1 has latency of 1000us.
 
     VoltDB determines a global ordering of transactions, and this schedule determines the level of concurrency.
     For example, there are 6 possible orderings of T_1, T_2 and T_3.
 
     Orderings T_2 <- T_3 <- T_1, T_3 <- T_2 <- T_1, T_1 <- T_2 <- T_3, T_1 <- T_3 <- T_2 have the lowest latency (max(500us, 1000us + 500us) = 1500us).
     Orderings T_2 <- T_1 <- T_3, T_3 <- T_1 <- T_2 have the highest latency (500us + 1000us + 500us = 2000us).
 
     There is 2/3 probability of the lowest latency schedule and a 1/3 probability of the highest latency schedule. Therefore the expected workload execution
     time is (2/3)*1500 + (1/3)*2000 = 1666us.

     Defined below are two functions EstimateBestCaseExecTime() and EstimateWorstCaseExecTime(). To calculate the expected execution time, we would need to find
     a parameter 'a' such that the expected execution time for the workload is calculated as a * EstimateBestCaseExecTime() + (1-a) * EstimateWorstCaseExecTime().

 Q: We can directly calculate a from the tree in EstimateBestCaseExecTime(), but I think this may be expensive. What heuristics might we use to get a?
    One simple idea (I think this bad but I'll record it here anyways) might be to set a to be the # of single-sited txns in W / # of txns in W.

 /// Get a best case estimation of workload execution time
 /// Idea: Get an estimation of workload execution time of a schedule with maximum possible concurrent execution of transactions.
 ///       * From "Assertion 1", two transactions which use disjoint sets of sites may execute concurrently.
 Define EstimateBestCaseExecTime(W):
    sort transactions in W in descending order of number of sites used

    // Build a tree of transactions in which for a given node, all descendants are scheduled before the transaction
    // of the current node. Siblings of the node may execute concurrently with it.

    // root is a special node 
    root = node( txn=null, parent=null, children=[] )
    foreach transaction t in W:
        parent = root
        inserted = False

        while not inserted:
            if parent.children is empty:
                parent.children.add(node( txn=t, parent=parent, children=[] ))
                inserted = True
            else:
                conflictNodes = []

                foreach node n in parent.children:
                    if intersection(SPS(t.sp), SPS(n.txn)) is not empty:
                        conflictNodes.add(n)

                if conflictNodes is empty:
                    parent.children.add(node( txn=t, parent=parent, children=[] ))
                    inserted = True
                elif conflictNodes.size == 1 and SPS(t.sp) < SPS(conflictNodes[0].txn.sp):
                    parent = conflictNodes[0]
                else:
                    remove conflictNodes from parent.children
                    parent.children.add(node( txn=t, parent=parent, children=conflictNodes ))
                    inserted = True

    // Determine the execution time of the schedule given by the tree
    execTime = lambda(node): return node.txn.lat + max execTime of nodes in node.children
    return execTime(root)

 /// Get a worst case estimation of the workload execution time
 Define EstimateWorstCaseExecTime(W):
    // Union-find data structure - a set of disjoint sets of sites. 
    // Each set of sites contains a pointer to the set of transactions which execute serially.
    siteSets = {}

    // for each site, create a set with an empty set of serially executing transactions.
    foreach site s in S:
        siteSets.addSet(s, {})

    foreach transaction t in W:
        sitesUsed = SPS(t.sp)

        if sitesUsed.size == 1:
            // Single-sited transaction, add to the list of transactions for the set containing the site.
            siteSet = siteSets.find(sitesUsed[0])
            siteSet.txns = siteSet.txns U {t}
        else:
            // Multi-site transaction, merge the sets containing sites the transaction executes at. 
            siteSet = siteSets.find(sitesUsed[0])
            foreach site in SPS(t.sp) / {sitesUsed[0]}:
                siteSet' = siteSets.find(site)
                if siteSet != siteSet':
                    siteSets.mergeSets(siteSet, siteSets)
            siteSet.txns = siteSet.txns U {t}

    foreach siteSet in siteSets:
        foreach transaction t in siteSet.txns:
            siteSet.txnsExecTime += t.lat

    return max txnsExecTime from siteSets

Model 2
=======
* Generate optimum repartitioning by solving as ILP problem
* Statistics collected for each transaction T_i:
  1. Stored Procedure
  2. Sites Used
  3. Initiator Host
  4. Observed Execution Latency (call it "oblat_i")
  5. For each Site S_j in "Sites Used", the total network cost associated with communicating with that Site. (call it "netlat_ij")
     * If when the transaction was observed, the site was remote then record the actual latency.
     * Otherwise, this latency needs to be estimated.

  Then the "Local" Execution Latency of the transaction is the Observed Execution Latency - the maximum of all network costs of communicating with remote sites.
  (call it "loclat_i")

* Ex.

Two hosts { H_1, H_2 }
Four sites { S_1, S_2, S_3, S_4 }
Transactions { T_1, T_2, T_3, T_4 }
Site assignments: H_1: { S_1, S_2 }, H_2: { S_3, S_4 }

T_1 touches { S_1, S_2, S_3 } and the initiator was host H_1
T_2 touches { S_2, S_3 } and the initiator was host H_2
T_3 touches { S_2 } and the initiator was host H_1
T_4 touches { S_4 } and the initiator was host H_2

oblat_1   = 5.05ms
netlat_11 = 2.00ms (estimated)
netlat_12 = 2.00ms (estimated)
netlat_13 = 5.00ms (actual)
netlat_14 = 0.00ms (unused)
loclat_1  = 0.05ms

oblat_2   = 2.52ms
netlat_21 = 0.00ms (unused)
netlat_22 = 2.00ms (actual)
netlat_23 = 2.50ms (estimated)
netlat_24 = 0.00ms (unused)
loclat_2  = 0.02ms

oblat_3   = 0.04ms
netlat_31 = 0.00ms (unused)
netlat_32 = 5.00ms (estimated) 
netlat_33 = 0.00ms (unused)
netlat_34 = 0.00ms (unused)
loclat_3  = 0.04ms

oblat_4   = 0.01ms
netlat_41 = 0.00ms (unused)
netlat_42 = 0.00ms (unused)
netlat_43 = 0.00ms (unused)
netlat_44 = 2.00ms (estimated)
loclat_4  = 0.01ms

Best Schedule: T_4 <- T_3 <- T_2 <- T_1
Worst Schedule: T_4 <- T_3 <- T_2 <- T_1

Let lat_1 be the latency of transaction T_1 when run on the optimum partitioning.
Let lat_2 be the latency of transaction T_2 when run on the optimum partitioning.
Let lat_3 be the latency of transaction T_3 when run on the optimum partitioning.
Let lat_4 be the latency of transaction T_4 when run on the optimum partitioning.

Constraint: lat_1 = loclat_1 + max( netlat_11 * a_12 + netlat_12 * a_22 + netlat_13 * a_32 )
Constraint: lat_2 = loclat_2 + max( netlat_22 * a_21 + netlat_23 * a_31 )
Constraint: lat_3 = loclat_3 + netlat_32 * a_32
Constraint: lat_4 = loclat_4 + netlat_44 * a_41

Minimize max( lat_4, lat_1 + lat_2 + lat_3 )

Let a_ij's, 0 <= a_ij <= 1 for all i,j give the assignment of sites to hosts (a_ij = 1 iff site s_i assigned to host h_j).

a_11 a_12
a_21 a_22
a_31 a_32
a_41 a_42

Constraint: a_11 + a_12 = 1
Constraint: a_21 + a_22 = 1
Constraint: a_31 + a_32 = 1
Constraint: a_41 + a_42 = 1
Constraint: 0 <= a_11 + a_21 + a_31 + a_41 <= 2
Constraint: 0 <= a_12 + a_22 + a_32 + a_42 <= 2

References
* GLPK - https://www.gnu.org/software/glpk/#introduction
* Using GLPK - http://www-sop.inria.fr/members/Frederic.Giroire/teaching/ubinet/pdfs/exercises-solvers.pdf
* TODO: Find a Java front-end for GLPK (perhaps JavaILP - http://javailp.sourceforge.net/)
